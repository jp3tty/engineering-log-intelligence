#!/usr/bin/env python3
"""
Performance test script for Day 10: Elasticsearch Integration
Tests Vercel Function performance with log ingestion and search.
"""

import sys
import os
import time
import json
import asyncio
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any
import statistics

# Add the project root to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'engineering_log_intelligence'))

from api.models.log_entry import LogEntry
from api.services.elasticsearch_service import ElasticsearchService
from data_simulation.simulator import DataSimulator, create_default_config


class MockRequest:
    """Mock request object for testing Vercel Functions."""
    
    def __init__(self, method="GET", body=None, query_params=None, headers=None):
        self.method = method
        self.body = json.dumps(body) if body else None
        self.queryStringParameters = query_params or {}
        self.headers = headers or {}
    
    def get_json(self):
        return json.loads(self.body) if self.body else {}


def generate_test_logs(count: int) -> List[LogEntry]:
    """Generate test log entries for performance testing."""
    print(f"üìù Generating {count} test log entries...")
    
    # Use our data simulator to generate realistic logs
    config = create_default_config()
    config['total_logs'] = count
    config['enable_splunk'] = True
    config['enable_sap'] = True
    config['enable_application'] = True
    
    simulator = DataSimulator(config)
    logs = []
    
    for _ in range(count):
        log_data = simulator.generate_log()
        log_entry = LogEntry.from_dict(log_data)
        logs.append(log_entry)
    
    print(f"‚úÖ Generated {len(logs)} test log entries")
    return logs


def test_elasticsearch_indexing_performance():
    """Test Elasticsearch indexing performance."""
    print("\nüîç Testing Elasticsearch Indexing Performance...")
    
    try:
        # Mock environment variables
        os.environ['ELASTICSEARCH_URL'] = 'http://localhost:9200'
        os.environ['ELASTICSEARCH_INDEX'] = 'logs_test'
        
        es_service = ElasticsearchService()
        
        # Generate test logs
        test_logs = generate_test_logs(1000)
        
        # Test single document indexing
        print("  üìÑ Testing single document indexing...")
        single_times = []
        
        for i in range(10):  # Test 10 single documents
            start_time = time.time()
            try:
                es_service.index_log_entry(test_logs[i])
                end_time = time.time()
                single_times.append(end_time - start_time)
            except Exception as e:
                print(f"    ‚ö†Ô∏è Single indexing failed: {e}")
        
        if single_times:
            avg_single_time = statistics.mean(single_times)
            print(f"    ‚úÖ Average single document indexing time: {avg_single_time:.4f}s")
        
        # Test bulk indexing
        print("  üìö Testing bulk document indexing...")
        bulk_times = []
        
        for i in range(5):  # Test 5 bulk operations
            batch_size = 100
            batch_logs = test_logs[i * batch_size:(i + 1) * batch_size]
            
            start_time = time.time()
            try:
                success_count, failed_count = es_service.bulk_index_log_entries(batch_logs)
                end_time = time.time()
                bulk_times.append(end_time - start_time)
                print(f"    üì¶ Batch {i+1}: {success_count} documents in {end_time - start_time:.4f}s")
            except Exception as e:
                print(f"    ‚ö†Ô∏è Bulk indexing failed: {e}")
        
        if bulk_times:
            avg_bulk_time = statistics.mean(bulk_times)
            print(f"    ‚úÖ Average bulk indexing time (100 docs): {avg_bulk_time:.4f}s")
            print(f"    üìä Bulk indexing rate: {100/avg_bulk_time:.2f} docs/second")
        
        return True
        
    except Exception as e:
        print(f"    ‚ùå Elasticsearch indexing test failed: {e}")
        return False


def test_elasticsearch_search_performance():
    """Test Elasticsearch search performance."""
    print("\nüîç Testing Elasticsearch Search Performance...")
    
    try:
        # Mock environment variables
        os.environ['ELASTICSEARCH_URL'] = 'http://localhost:9200'
        os.environ['ELASTICSEARCH_INDEX'] = 'logs_test'
        
        es_service = ElasticsearchService()
        
        # Test different search scenarios
        search_tests = [
            {
                "name": "Text search",
                "params": {"query_text": "error", "limit": 50}
            },
            {
                "name": "Level filter",
                "params": {"level": "ERROR", "limit": 50}
            },
            {
                "name": "Source type filter",
                "params": {"source_type": "application", "limit": 50}
            },
            {
                "name": "Time range filter",
                "params": {
                    "start_time": datetime.now(timezone.utc) - timedelta(hours=1),
                    "end_time": datetime.now(timezone.utc),
                    "limit": 50
                }
            },
            {
                "name": "Complex search",
                "params": {
                    "query_text": "database",
                    "level": "ERROR",
                    "source_type": "application",
                    "limit": 50
                }
            }
        ]
        
        search_times = []
        
        for test in search_tests:
            print(f"  üîç Testing {test['name']}...")
            
            times = []
            for _ in range(5):  # Run each test 5 times
                start_time = time.time()
                try:
                    result = es_service.search_logs(**test['params'])
                    end_time = time.time()
                    times.append(end_time - start_time)
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Search failed: {e}")
                    break
            
            if times:
                avg_time = statistics.mean(times)
                search_times.append(avg_time)
                print(f"    ‚úÖ Average search time: {avg_time:.4f}s")
                print(f"    üìä Search rate: {test['params']['limit']/avg_time:.2f} results/second")
        
        if search_times:
            overall_avg = statistics.mean(search_times)
            print(f"  üìä Overall average search time: {overall_avg:.4f}s")
        
        return True
        
    except Exception as e:
        print(f"    ‚ùå Elasticsearch search test failed: {e}")
        return False


def test_vercel_function_performance():
    """Test Vercel Function performance with mock requests."""
    print("\n‚ö° Testing Vercel Function Performance...")
    
    try:
        # Test log ingestion function
        print("  üì• Testing log ingestion function...")
        
        # Generate test logs
        test_logs = generate_test_logs(100)
        
        # Create mock request
        request_body = {
            "logs": [log.to_dict() for log in test_logs[:10]]  # Test with 10 logs
        }
        
        request = MockRequest(method="POST", body=request_body)
        
        # Import and test the function
        try:
            from api.logs.ingest_enhanced import handler as ingest_handler
            
            times = []
            for _ in range(5):  # Run 5 times
                start_time = time.time()
                try:
                    response = ingest_handler(request)
                    end_time = time.time()
                    times.append(end_time - start_time)
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Ingest function failed: {e}")
                    break
            
            if times:
                avg_time = statistics.mean(times)
                print(f"    ‚úÖ Average ingestion time: {avg_time:.4f}s")
                print(f"    üìä Ingestion rate: {10/avg_time:.2f} logs/second")
        
        except ImportError as e:
            print(f"    ‚ö†Ô∏è Could not import ingest function: {e}")
        
        # Test search function
        print("  üîç Testing search function...")
        
        search_params = {
            "q": "error",
            "limit": "50",
            "source_type": "application"
        }
        
        request = MockRequest(method="GET", query_params=search_params)
        
        try:
            from api.logs.search_enhanced import handler as search_handler
            
            times = []
            for _ in range(5):  # Run 5 times
                start_time = time.time()
                try:
                    response = search_handler(request)
                    end_time = time.time()
                    times.append(end_time - start_time)
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Search function failed: {e}")
                    break
            
            if times:
                avg_time = statistics.mean(times)
                print(f"    ‚úÖ Average search time: {avg_time:.4f}s")
                print(f"    üìä Search rate: {50/avg_time:.2f} results/second")
        
        except ImportError as e:
            print(f"    ‚ö†Ô∏è Could not import search function: {e}")
        
        return True
        
    except Exception as e:
        print(f"    ‚ùå Vercel Function performance test failed: {e}")
        return False


def test_memory_usage():
    """Test memory usage during operations."""
    print("\nüíæ Testing Memory Usage...")
    
    try:
        import psutil
        import gc
        
        # Get initial memory usage
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"  üìä Initial memory usage: {initial_memory:.2f} MB")
        
        # Generate large number of logs
        print("  üìù Generating 10,000 logs...")
        test_logs = generate_test_logs(10000)
        
        # Check memory after generation
        gc.collect()
        after_generation = process.memory_info().rss / 1024 / 1024
        print(f"  üìä Memory after generation: {after_generation:.2f} MB")
        print(f"  üìä Memory increase: {after_generation - initial_memory:.2f} MB")
        
        # Test bulk operations
        print("  üìö Testing bulk operations...")
        
        # Mock environment variables
        os.environ['ELASTICSEARCH_URL'] = 'http://localhost:9200'
        os.environ['ELASTICSEARCH_INDEX'] = 'logs_test'
        
        try:
            es_service = ElasticsearchService()
            
            # Test bulk indexing
            start_memory = process.memory_info().rss / 1024 / 1024
            es_service.bulk_index_log_entries(test_logs[:1000])
            end_memory = process.memory_info().rss / 1024 / 1024
            
            print(f"  üìä Memory before bulk indexing: {start_memory:.2f} MB")
            print(f"  üìä Memory after bulk indexing: {end_memory:.2f} MB")
            print(f"  üìä Memory increase: {end_memory - start_memory:.2f} MB")
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Elasticsearch operations failed: {e}")
        
        # Clean up
        del test_logs
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024
        print(f"  üìä Final memory usage: {final_memory:.2f} MB")
        print(f"  üìä Total memory increase: {final_memory - initial_memory:.2f} MB")
        
        return True
        
    except ImportError:
        print("    ‚ö†Ô∏è psutil not available, skipping memory test")
        return True
    except Exception as e:
        print(f"    ‚ùå Memory usage test failed: {e}")
        return False


def test_concurrent_operations():
    """Test concurrent operations performance."""
    print("\nüîÑ Testing Concurrent Operations...")
    
    try:
        import threading
        import queue
        
        # Mock environment variables
        os.environ['ELASTICSEARCH_URL'] = 'http://localhost:9200'
        os.environ['ELASTICSEARCH_INDEX'] = 'logs_test'
        
        es_service = ElasticsearchService()
        
        # Generate test data
        test_logs = generate_test_logs(500)
        
        # Test concurrent indexing
        print("  üìù Testing concurrent indexing...")
        
        def index_logs(logs, results_queue):
            start_time = time.time()
            try:
                success_count, failed_count = es_service.bulk_index_log_entries(logs)
                end_time = time.time()
                results_queue.put({
                    'success': success_count,
                    'failed': failed_count,
                    'time': end_time - start_time
                })
            except Exception as e:
                results_queue.put({'error': str(e)})
        
        # Create threads
        threads = []
        results_queue = queue.Queue()
        batch_size = 100
        
        for i in range(5):  # 5 concurrent operations
            batch_logs = test_logs[i * batch_size:(i + 1) * batch_size]
            thread = threading.Thread(target=index_logs, args=(batch_logs, results_queue))
            threads.append(thread)
        
        # Start all threads
        start_time = time.time()
        for thread in threads:
            thread.start()
        
        # Wait for all threads to complete
        for thread in threads:
            thread.join()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Collect results
        results = []
        while not results_queue.empty():
            results.append(results_queue.get())
        
        # Analyze results
        successful_operations = [r for r in results if 'success' in r]
        failed_operations = [r for r in results if 'error' in r]
        
        if successful_operations:
            total_success = sum(r['success'] for r in successful_operations)
            total_failed = sum(r['failed'] for r in successful_operations)
            avg_operation_time = statistics.mean(r['time'] for r in successful_operations)
            
            print(f"    ‚úÖ Successful operations: {len(successful_operations)}")
            print(f"    ‚úÖ Total documents indexed: {total_success}")
            print(f"    ‚úÖ Total documents failed: {total_failed}")
            print(f"    ‚úÖ Average operation time: {avg_operation_time:.4f}s")
            print(f"    ‚úÖ Total concurrent time: {total_time:.4f}s")
            print(f"    üìä Concurrent indexing rate: {total_success/total_time:.2f} docs/second")
        
        if failed_operations:
            print(f"    ‚ö†Ô∏è Failed operations: {len(failed_operations)}")
            for result in failed_operations:
                print(f"      Error: {result['error']}")
        
        return True
        
    except Exception as e:
        print(f"    ‚ùå Concurrent operations test failed: {e}")
        return False


def main():
    """Run all Day 10 performance tests."""
    print("üöÄ Day 10: Elasticsearch Integration - Performance Test Suite")
    print("=" * 70)
    
    tests = [
        ("Elasticsearch Indexing Performance", test_elasticsearch_indexing_performance),
        ("Elasticsearch Search Performance", test_elasticsearch_search_performance),
        ("Vercel Function Performance", test_vercel_function_performance),
        ("Memory Usage", test_memory_usage),
        ("Concurrent Operations", test_concurrent_operations)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nüß™ Running {test_name}...")
        try:
            if test_func():
                passed += 1
                print(f"‚úÖ {test_name} PASSED")
            else:
                print(f"‚ùå {test_name} FAILED")
        except Exception as e:
            print(f"‚ùå {test_name} FAILED with error: {e}")
        print("-" * 70)
    
    print(f"\nüìä Performance Test Results: {passed}/{total} test suites passed")
    
    if passed == total:
        print("üéâ All Day 10 performance tests PASSED! Elasticsearch integration is performing well.")
        print("\nüìã Day 10 Performance Achievements:")
        print("  ‚úÖ Elasticsearch indexing performance tested")
        print("  ‚úÖ Elasticsearch search performance tested")
        print("  ‚úÖ Vercel Function performance tested")
        print("  ‚úÖ Memory usage optimized")
        print("  ‚úÖ Concurrent operations tested")
        return True
    else:
        print("‚ö†Ô∏è  Some performance tests FAILED. Please review the issues above.")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
