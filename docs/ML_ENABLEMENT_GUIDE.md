# ML Feature Enablement Guide

**Date:** October 11, 2025  
**Version:** 1.0  
**Status:** Ready for ML Activation

## üéØ Overview

This guide provides step-by-step instructions for enabling the Machine Learning features in the Engineering Log Intelligence System. The ML infrastructure is already built and tested, but currently returns mock data. This guide will help you activate real ML inference.

## üìã Prerequisites

Before enabling ML features, ensure you have:

- ‚úÖ Python 3.9+ installed
- ‚úÖ All dependencies from `requirements.txt` installed
- ‚úÖ Access to the production database (PostgreSQL on Railway)
- ‚úÖ Sufficient log data for model training (minimum 1,000 logs recommended)
- ‚úÖ Environment variables configured in Vercel

## üèóÔ∏è ML Architecture Status

### Current ML Components

The following ML components are already implemented:

#### 1. **ML Models** (`external-services/ml/`)
- ‚úÖ `log_classifier.py` - Log classification model (8 categories)
- ‚úÖ `anomaly_detector.py` - Anomaly detection model
- ‚úÖ `ml_service.py` - ML service coordinator
- ‚úÖ `real_time_processor.py` - Real-time processing engine
- ‚úÖ `ab_testing.py` - A/B testing framework
- ‚úÖ `ml_monitoring.py` - Model performance monitoring

#### 2. **API Endpoints** (`api/ml.py`)
- ‚úÖ `/api/ml?action=analyze` - Log analysis and classification
- ‚úÖ `/api/ml?action=realtime` - Real-time processing control
- ‚úÖ `/api/ml?action=abtest` - A/B testing management
- ‚úÖ `/api/ml?action=status` - ML model status

#### 3. **Current Status**
- üü° **Mock Data Mode**: API returns simulated ML results
- üü¢ **Infrastructure Ready**: All components built and tested
- üü¢ **Documentation Complete**: Full ML documentation available

## üöÄ Enablement Steps

### Step 1: Verify Training Data

First, verify you have sufficient log data for training:

```bash
cd /Users/jeremypetty/Documents/projects/tools_developer/engineering_log_intelligence

# Check database for log count
python check_database.py
```

**Expected Output:**
- Minimum 1,000 logs (basic training)
- Recommended 10,000+ logs (production-ready models)

### Step 2: Train ML Models

Train the ML models using your production log data:

```bash
# Activate virtual environment
source venv/bin/activate

# Run model training script
python -c "
from external_services.ml.ml_service import MLService
from src.api.database import get_logs_for_training

# Initialize ML service
ml_service = MLService(model_storage_path='models/')

# Get training data from database
training_data = get_logs_for_training(limit=10000)

# Train all models
results = ml_service.train_all_models(training_data)

print('Training Results:')
print(f'Classification Model Accuracy: {results[\"models\"][\"log_classifier\"][\"accuracy\"]}')
print(f'Anomaly Detection Model Accuracy: {results[\"models\"][\"anomaly_detector\"][\"accuracy\"]}')
print('Models saved successfully!')
"
```

### Step 3: Update API to Use Real Models

Modify `api/ml.py` to use real ML models instead of mock data:

```python
# Add at the top of api/ml.py
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from external_services.ml.ml_service import MLService

# Initialize ML service (add after imports)
ml_service = MLService(model_storage_path='models/')
ml_service.initialize_models()

# Update handle_analyze() method to use real ML:
def handle_analyze(self, data=None):
    if data and data.get('log_data'):
        log_data = data.get('log_data', [])
        results = []
        
        for log in log_data[:10]:
            # Use real ML analysis instead of mock data
            analysis = ml_service.analyze_log(log)
            results.append({
                "log_id": log.get('id'),
                "classification": analysis['analysis']['classification']['category'],
                "confidence": analysis['analysis']['classification']['confidence'],
                "anomaly_score": analysis['analysis']['anomaly'].get('confidence', 0),
                "is_anomaly": analysis['analysis']['anomaly'].get('is_anomaly', False),
                "timestamp": datetime.utcnow().isoformat()
            })
        
        return {
            "success": True,
            "results": results,
            "total_analyzed": len(results),
            "timestamp": datetime.utcnow().isoformat()
        }
```

### Step 4: Upload Trained Models to Vercel

The trained models need to be accessible to Vercel Functions. Options:

#### Option A: Include Models in Deployment (Recommended for Small Models)
```bash
# Ensure models directory is not in .gitignore
# Add models to git (if they're small enough, <10MB)
git add models/
git commit -m "Add trained ML models"
git push

# Deploy to Vercel
vercel --prod
```

#### Option B: Use External Model Storage (Recommended for Large Models)
```bash
# Upload models to AWS S3 or similar storage
aws s3 cp models/log_classifier.pkl s3://your-bucket/models/
aws s3 cp models/anomaly_detector.pkl s3://your-bucket/models/

# Add S3 credentials to Vercel environment variables
vercel env add MODEL_STORAGE_URL
# Enter: s3://your-bucket/models/

vercel env add AWS_ACCESS_KEY_ID
vercel env add AWS_SECRET_ACCESS_KEY
```

### Step 5: Configure Environment Variables

Add ML-specific environment variables to Vercel:

```bash
# Set ML feature flag
vercel env add ML_ENABLED
# Enter: true

# Set model paths
vercel env add ML_MODEL_PATH
# Enter: ./models/ (if included) or s3://bucket/models/ (if external)

# Set ML thresholds
vercel env add ML_ANOMALY_THRESHOLD
# Enter: 0.7

vercel env add ML_CLASSIFICATION_MIN_CONFIDENCE
# Enter: 0.6
```

### Step 6: Test ML Endpoints

Test the ML endpoints to verify they're working:

```bash
# Test ML analysis endpoint
curl -X POST https://your-app.vercel.app/api/ml \
  -H "Content-Type: application/json" \
  -d '{
    "action": "analyze",
    "log_data": [{
      "id": "test_001",
      "message": "ERROR: Database connection failed",
      "timestamp": "2025-10-11T10:00:00Z"
    }]
  }'

# Expected response with real ML analysis
```

### Step 7: Monitor ML Performance

Monitor ML model performance using the built-in monitoring:

```bash
# Check ML model status
curl https://your-app.vercel.app/api/ml?action=status

# Check model metrics
python -c "
from external_services.ml.ml_monitoring import MLMonitor

monitor = MLMonitor()
metrics = monitor.get_model_metrics('log_classifier')
print(f'Model Performance:')
print(f'  Accuracy: {metrics[\"accuracy\"]}')
print(f'  Latency: {metrics[\"avg_latency_ms\"]}ms')
print(f'  Predictions: {metrics[\"total_predictions\"]}')
"
```

## üìä ML Feature Capabilities

Once enabled, the following ML features will be active:

### 1. **Log Classification**
- Automatically categorizes logs into 8 categories:
  - Security
  - Performance
  - System
  - Application
  - Database
  - Network
  - Authentication
  - Error
- Confidence scores for each prediction
- Multi-label classification support

### 2. **Anomaly Detection**
- Identifies unusual patterns in logs
- Detects 6 types of anomalies:
  - System failures
  - Security breaches
  - Performance issues
  - Data corruption
  - Network anomalies
  - Resource exhaustion
- Severity scoring (low/medium/high/critical)

### 3. **Real-time Processing**
- Processes logs as they arrive via Kafka
- Instant analysis and alerting
- Performance monitoring and health checks
- Automatic alerting for high-priority issues

### 4. **A/B Testing**
- Compare multiple model versions
- Automated winner selection
- Statistical significance testing
- Safe model deployment

## üîß Troubleshooting

### Issue: Models Not Loading
**Solution:**
```bash
# Check if models exist
ls -la models/

# Verify model files
python -c "
import os
print('Log Classifier:', os.path.exists('models/log_classifier.pkl'))
print('Anomaly Detector:', os.path.exists('models/anomaly_detector.pkl'))
"
```

### Issue: Low Model Accuracy
**Solution:**
- Train with more data (increase from 1,000 to 10,000+ logs)
- Verify data quality and diversity
- Consider retraining with different parameters

### Issue: Slow ML Inference
**Solution:**
- Enable caching for frequent predictions
- Use batch processing for multiple logs
- Consider upgrading Vercel plan for more memory

### Issue: Vercel Function Timeout
**Solution:**
```bash
# Vercel functions have 10s timeout on Hobby plan
# Optimize by:
# 1. Using smaller, faster models
# 2. Implementing async processing
# 3. Caching predictions
```

## üìà Performance Expectations

### Model Accuracy (After Training with 10,000+ Logs)
- Log Classification: 85-95% accuracy
- Anomaly Detection: 80-90% accuracy

### Inference Speed
- Single log analysis: 50-200ms
- Batch processing (10 logs): 100-500ms
- Real-time throughput: 1,000+ logs/minute

### Resource Usage
- Memory: 256-512 MB per Vercel Function
- Storage: 10-50 MB for trained models
- Database queries: 1-5 per analysis

## üéØ Next Steps After Enablement

1. **Monitor Model Performance**
   - Track accuracy metrics daily
   - Review false positives/negatives
   - Retrain models monthly with new data

2. **Enable Real-time Processing**
   - Start real-time processor for live analysis
   - Configure alerting thresholds
   - Set up notification channels

3. **Implement A/B Testing**
   - Test model improvements safely
   - Compare different algorithms
   - Roll out better models gradually

4. **Optimize Performance**
   - Profile ML inference latency
   - Implement caching strategies
   - Consider model quantization for speed

## üìö Related Documentation

- [Day 17: ML Pipeline Integration](DAY17_ML_INTEGRATION.md)
- [Day 18: Real-time Processing](DAY18_REALTIME_PROCESSING.md)
- [Day 19: A/B Testing Framework](DAY19_AB_TESTING.md)
- [ML Model Documentation](../external-services/ml/README.md)
- [API Reference](API_REFERENCE.md)

## üÜò Support

If you encounter issues during ML enablement:

1. Check the troubleshooting section above
2. Review ML-related documentation (Days 17-19)
3. Check Vercel function logs: `vercel logs`
4. Verify environment variables: `vercel env ls`

---

**Last Updated:** October 11, 2025  
**Status:** Ready for Implementation  
**Estimated Time:** 2-3 hours for full enablement

